{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import yaml\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from Levenshtein import distance\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/storage/3020/KrivorotovI/db/HKR/')\n",
    "from htrl.dataset import ImageDataset\n",
    "from htrl.crnn import CRNN\n",
    "from htrl.utils import strLabelConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "\n",
    "    def __init__(self, device, save_folder,\n",
    "                 train_dataset, valid_dataset, test1_dataset, test2_dataset, num_imgs=-1,\n",
    "                 model_params={}, num_workers=8, batch_size=4, seed=34, max_epochs=1, model_pretrain='',\n",
    "                 scheduler_params={}, optimizer_params={}):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = torch.device(device)\n",
    "        self.save_folder = save_folder\n",
    "\n",
    "        self.train_dataset = train_dataset\n",
    "        self.valid_dataset = valid_dataset\n",
    "        self.test1_dataset = test1_dataset\n",
    "        self.test2_dataset = test2_dataset\n",
    "        self.num_imgs = num_imgs\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.seed = seed\n",
    "        self.batch_size = batch_size\n",
    "        self.max_epochs = max_epochs\n",
    "        \n",
    "        self.model_params = model_params\n",
    "        \n",
    "        self.model = CRNN(**self.model_params).to(self.device)\n",
    "        if model_pretrain is not '':\n",
    "            self.model.load_state_dict(torch.load(model_pretrain, map_location=self.device))\n",
    "            print(f'Successfully loaded model from {model_pretrain}')\n",
    "        self.loss = nn.CTCLoss()\n",
    "        self.optimizer_params = optimizer_params\n",
    "        self.optimizer = torch.optim.RMSprop(self.model.parameters(), **self.optimizer_params)\n",
    "        self.scheduler_params = scheduler_params\n",
    "        self.scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, **self.scheduler_params)\n",
    "\n",
    "        self.initialize_training()\n",
    "        self.prepare_dirs()\n",
    "\n",
    "    def initialize_training(self):\n",
    "        self.patience = 0\n",
    "        self.epoch = 0\n",
    "        self.score_best = None\n",
    "\n",
    "        self.fix_seeds()\n",
    "        \n",
    "        train_dataset = ImageDataset(pickle_file=self.train_dataset,\n",
    "                                     meta=False, num_imgs=self.num_imgs)\n",
    "        self.train_iterator = DataLoader(dataset=train_dataset, batch_size=self.batch_size, \n",
    "                                         shuffle=True, num_workers=self.num_workers, drop_last = True)\n",
    "        \n",
    "        valid_dataset = ImageDataset(pickle_file=self.valid_dataset,\n",
    "                                     meta=False, num_imgs=self.num_imgs)\n",
    "        self.valid_iterator = DataLoader(dataset=valid_dataset, batch_size=self.batch_size, \n",
    "                                         shuffle=False, num_workers=self.num_workers, drop_last = True)\n",
    "        \n",
    "        test1_dataset = ImageDataset(pickle_file=self.test1_dataset,\n",
    "                                     meta=False, num_imgs=self.num_imgs)\n",
    "        self.test1_iterator = DataLoader(dataset=test1_dataset, batch_size=self.batch_size, \n",
    "                                         shuffle=False, num_workers=self.num_workers, drop_last = True)\n",
    "        \n",
    "        test2_dataset = ImageDataset(pickle_file=self.test2_dataset,\n",
    "                                     meta=False, num_imgs=self.num_imgs)\n",
    "        self.test2_iterator = DataLoader(dataset=test2_dataset, batch_size=self.batch_size, \n",
    "                                         shuffle=False, num_workers=self.num_workers, drop_last = True)\n",
    "\n",
    "        with open(self.train_dataset, 'rb') as f:\n",
    "            full_dataset = pickle.load(f)\n",
    "        alphabet = ''\n",
    "        for example in full_dataset:\n",
    "            alphabet += example['description']\n",
    "        alphabet = list(set(alphabet))\n",
    "        self.alphabet =''.join(alphabet)\n",
    "        self.converter = strLabelConverter(self.alphabet)\n",
    "\n",
    "\n",
    "    def prepare_dirs(self):\n",
    "        if not os.path.exists(self.save_folder):\n",
    "            os.makedirs(self.save_folder)\n",
    "\n",
    "    def get_parameters(self):\n",
    "        params = {\n",
    "                    \"device\": str(self.device),\n",
    "                    \"save_folder\": self.save_folder,\n",
    "                    \"train_dataset\": self.train_dataset,\n",
    "                    \"valid_dataset\": self.valid_dataset,\n",
    "                    \"test1_dataset\": self.test1_dataset,\n",
    "                    \"test2_dataset\": self.test2_dataset,\n",
    "                    \"num_imgs\": self.num_imgs,\n",
    "                    \"num_workers\": self.num_workers,\n",
    "                    \"seed\": self.seed,\n",
    "                    \"batch_size\": self.batch_size,\n",
    "                    \"model_params\": self.model_params,\n",
    "                    \"max_epochs\": self.max_epochs,\n",
    "        }\n",
    "        return params\n",
    "\n",
    "    def fix_seeds(self):\n",
    "        torch.manual_seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        \n",
    "    def train_on_batches(self):\n",
    "        self.model.train()\n",
    "        while True:\n",
    "            with tqdm(total=len(self.train_iterator)) as bar_train:\n",
    "                for x, y_true in self.train_iterator:\n",
    "                    '''TRAINING CODE HERE'''\n",
    "                    pred_text = self.model(x.to(self.device))\n",
    "                    preds_size = torch.LongTensor([pred_text.size(0)] * self.batch_size)\n",
    "\n",
    "                    t_text, l_text = self.converter.encode(list(y_true))\n",
    "\n",
    "                    loss = self.loss(pred_text, t_text, preds_size, l_text)\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    _, decode_text = pred_text.max(2)\n",
    "                    decode_text = decode_text.transpose(1, 0).contiguous().view(-1)\n",
    "                    decode_text = self.converter.decode(decode_text.data, preds_size, raw=False)\n",
    "\n",
    "                    char_correct = 0\n",
    "                    total_char = 0\n",
    "                    for pred, gt in zip(decode_text, y_true):\n",
    "                        for i, pred_char in enumerate(pred):\n",
    "                            if i<len(gt):\n",
    "                                if pred_char == gt[i]:\n",
    "                                    char_correct += 1\n",
    "                        total_char += len(gt)\n",
    "\n",
    "                    acc_char = (char_correct)/float(total_char)\n",
    "\n",
    "                    total_str = 0\n",
    "                    n_correct = 0\n",
    "                    cer = 0\n",
    "                    for pred, target in zip(decode_text, y_true):\n",
    "                        if pred == target:\n",
    "                            n_correct += 1\n",
    "                        else:\n",
    "                            cer += distance(pred, target)/len(target)\n",
    "                        total_str+=1\n",
    "\n",
    "                    acc_str = (n_correct) / total_str\n",
    "                    cer = cer/total_str\n",
    "                    '''TRAINING CODE HERE'''\n",
    "                    bar_train.set_description(\n",
    "                        f\"Epoch: {self.epoch:3}. Current CER: {cer:8.7}\")\n",
    "                    bar_train.update(1)\n",
    "\n",
    "            self.epoch += 1\n",
    "            self.scheduler.step()\n",
    "            self.validate()\n",
    "            if 0 < self.max_epochs <= self.epoch:\n",
    "                break\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        try:\n",
    "            self.train_on_batches()\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Stopped training\")\n",
    "        finally:\n",
    "            self.save(save_policy='last')\n",
    "    \n",
    "    def validate(self):\n",
    "        val_epoch_loss = []\n",
    "        val_epoch_acc_char = []\n",
    "        val_epoch_acc_str = []\n",
    "        val_epoch_cer = []\n",
    "        with tqdm(total = len(self.valid_iterator)) as bar_val:\n",
    "            for i, (images, gt_text) in enumerate(self.valid_iterator):\n",
    "                results = self.validate_on_batches(images, gt_text)\n",
    "                loss, acc_char, acc_str, cer, decode_text, gt_text, raw_preds = results\n",
    "                \n",
    "                val_epoch_loss.append(loss)\n",
    "                val_epoch_acc_char.append(acc_char)\n",
    "                val_epoch_acc_str.append(acc_str)\n",
    "                val_epoch_cer.append(cer)\n",
    "                \n",
    "                printed_data = f\"Ep: {self.epoch}. Val loss: {np.mean(val_epoch_loss):.4f}.\\\n",
    "                Acc char: {np.mean(val_epoch_acc_char):.4f}.\\\n",
    "                Acc str: {np.mean(val_epoch_acc_str):.4f}.\\\n",
    "                CER: {np.mean(val_epoch_cer):.4f}.\"\n",
    "                bar_val.set_description(printed_data)\n",
    "                bar_val.update(1)\n",
    "        if self.score_best is None:\n",
    "            self.score_best = np.mean(val_epoch_loss)\n",
    "        else:\n",
    "            if self.score_best > np.mean(val_epoch_loss):\n",
    "                self.score_best = np.mean(val_epoch_loss)\n",
    "                self.save(save_policy='best')\n",
    "                print(f'Model improved on valid with loss: {np.mean(val_epoch_loss)}')\n",
    "        for raw_pred, pred, gt in zip(raw_preds, decode_text, gt_text):\n",
    "            print('%-20s => %-20s, gt: %-20s' % (raw_pred, pred, gt)) \n",
    "        self.model.train()\n",
    "            \n",
    "    def validate_on_batches(self, images, gt_text):\n",
    "        \n",
    "        self.model.eval()\n",
    "        pred_text = self.model(images.to(self.device))\n",
    "\n",
    "        preds_size = torch.LongTensor([pred_text.size(0)] * self.batch_size)\n",
    "\n",
    "        t_text, l_text = self.converter.encode(list(gt_text))\n",
    "\n",
    "        loss = self.loss(pred_text, t_text, preds_size, l_text)\n",
    "\n",
    "        _, decode_text = pred_text.max(2)\n",
    "        decode_text = decode_text.transpose(1, 0).contiguous().view(-1)\n",
    "        decode_text = self.converter.decode(decode_text.data, preds_size, raw=False)\n",
    "\n",
    "        char_correct = 0\n",
    "        total_char = 0\n",
    "        for pred, gt in zip(decode_text, gt_text):\n",
    "            for i, pred_char in enumerate(pred):\n",
    "                if i<len(gt):\n",
    "                    if pred_char == gt[i]:\n",
    "                        char_correct += 1\n",
    "            total_char += len(gt)\n",
    "\n",
    "        acc_char = (char_correct)/float(total_char)\n",
    "\n",
    "        total_str = 0\n",
    "        n_correct = 0\n",
    "        cer = 0\n",
    "        for pred, target in zip(decode_text, gt_text):\n",
    "            if pred == target:\n",
    "                n_correct += 1\n",
    "            else:\n",
    "                cer += distance(pred, target)/len(target)\n",
    "            total_str+=1\n",
    "\n",
    "        acc_str = (n_correct)/total_str\n",
    "        cer = cer/total_str\n",
    "\n",
    "        _, preds = pred_text.max(2)\n",
    "        preds = preds.transpose(1, 0).contiguous().view(-1)\n",
    "        raw_preds = self.converter.decode(preds.data, preds_size, raw=True)\n",
    "\n",
    "        return (loss.item(), acc_char, acc_str, cer, decode_text[:5], gt_text[:5], raw_preds[:5])\n",
    "\n",
    "    def save(self, save_policy='best'):\n",
    "        print(f\"Saving trainer to {self.save_folder}.\")\n",
    "        if len(self.save_folder) > 0 and not os.path.exists(self.save_folder):\n",
    "            os.makedirs(self.save_folder)\n",
    "            \n",
    "        if save_policy == 'best':\n",
    "            torch.save(self.model.state_dict(), os.path.join(self.save_folder, \"model_state_dict\"))\n",
    "            torch.save(self.model, os.path.join(self.save_folder, \"model\"))\n",
    "        elif save_policy == 'last':\n",
    "            torch.save(self.model.state_dict(), os.path.join(self.save_folder, \"model_last_state_dict\"))\n",
    "            torch.save(self.model, os.path.join(self.save_folder, \"model_last\"))\n",
    "\n",
    "        torch.save({\n",
    "            \"parameters\": self.get_parameters()\n",
    "        }, os.path.join(self.save_folder, \"trainer\"))\n",
    "        print(\"Trainer is saved.\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, load_folder, device=\"cpu\", load_policy='best'):\n",
    "        checkpoint = torch.load(os.path.join(load_folder, \"trainer\"), map_location=device)\n",
    "        parameters = checkpoint[\"parameters\"]\n",
    "        parameters.pop(\"device\", None)\n",
    "        trainer = cls(device=device, **parameters)\n",
    "        if load_policy == 'best':\n",
    "            trainer.model = torch.load(os.path.join(load_folder, \"model\"))\n",
    "        elif load_policy == 'last':\n",
    "            trainer.model = torch.load(os.path.join(load_folder, \"model_last\"))\n",
    "        return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_dict = {'device': 'cuda:3',\n",
    "             'save_folder': '/home/storage/3020/KrivorotovI/db/HKR/test',\n",
    "             'train_dataset': '/home/storage/3020/KrivorotovI/db/HKR/datasets/train.pkl',\n",
    "             'valid_dataset': '/home/storage/3020/KrivorotovI/db/HKR/datasets/val.pkl',\n",
    "             'test1_dataset': '/home/storage/3020/KrivorotovI/db/HKR/datasets/test1.pkl',\n",
    "             'test2_dataset': '/home/storage/3020/KrivorotovI/db/HKR/datasets/test2.pkl',\n",
    "             'num_imgs': 64000, 'num_workers': 1, 'batch_size': 64, 'seed': 34, 'max_epochs': 10, 'model_pretrain': '',\n",
    "             'model_params':{'imgH': 32,\n",
    "                             'nc': 3,\n",
    "                             'nclass': 80,\n",
    "                             'nh': 256,\n",
    "                             'n_rnn': 2,\n",
    "                             'leakyRelu': False\n",
    "                             },\n",
    "             'optimizer_params':{'lr': 0.001},\n",
    "             'scheduler_params':{'milestones':[40, 70],\n",
    "                                 'gamma': 0.1}}\n",
    "trainer = Trainer(**init_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:105: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b0f263d1424501bc5491d51fee8203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=711.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
